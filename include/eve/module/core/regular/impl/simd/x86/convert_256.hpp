//==================================================================================================
/*
  EVE - Expressive Vector Engine
  Copyright : EVE Contributors & Maintainers
  SPDX-License-Identifier: MIT
*/
//==================================================================================================
#pragma once

#include <eve/as.hpp>
#include <eve/concept/value.hpp>
#include <eve/detail/implementation.hpp>

#include <type_traits>

namespace eve::detail
{
  //================================================================================================
  // 256 bits ==> 256/128 bits
  // We need to care only for AVX-sized input, which means we'll only convert by using a
  // Split/Compute/Combine skeletons (my PHD director would be proud :p).
  // We don't call aggregate cause aggregate wants EVERYTHING to be sliceable.
  //================================================================================================
  template<real_scalar_value In, typename N, real_scalar_value Out>
  EVE_FORCEINLINE wide<Out, N>
  convert_(EVE_SUPPORTS(avx_), wide<In, N> const &v0, as<Out> const &tgt) noexcept
    requires std::same_as<abi_t<In, N>, x86_256_>
  {
    if constexpr( std::is_same_v<In, Out> )
    {
      return v0;
    }
    else if constexpr( current_api >= avx2 && (N::value == 16)
                       && std::integral<In> && (sizeof(In) == 2)
                       && std::integral<Out> && (sizeof(Out) == 1) )
    {
      // u16 to u8
      const auto vand   = v0 & wide<In, N> {0xff};
      const auto [l, h] = vand.slice();
      return _mm_packus_epi16(l, h);
    }
    else if constexpr( current_api >= avx2 && (N::value == 4)
                       && std::integral<In> && (sizeof(In) == 8)
                       && std::integral<Out> && (sizeof(Out) == 4) )
    {
      // u64 to u32
      auto const p = _mm256_permutevar8x32_epi32(v0, _mm256_set_epi32(7, 5, 3, 1, 6, 4, 2, 0));
#ifdef __clang__
      // What's the difference between this clang specific version and the
      // generic one you may ask? That's a good question, especially because
      // _mm256_extractf128_si128 has a latency of 3 cycles...
      //
      // It turns out clang understands better the semantic of permutevar8x32 +
      // extractf128 than permutevar8x32 + castsi256_si128. We can see this by
      // looking at the LLVM IR generated by these two sequences: https://godbolt.org/z/K4dnTWPWo
      // With extractf128, we can see that clang understands that we are
      // truncating a vector of <4 x i64> to a vector of <4 x i32>. With
      // castsi256_si128, it generates a bunch of shuffles (which are still
      // semantically equivalent). Nevertheless, with only this code, we see
      // that the codegen in the end is the same.
      //
      // But interesting optimizations can then occur on more complex code,
      // like this one: https://godbolt.org/z/45sor8Ta1. In this case, we are
      // truncating and then zero-extending our vector, which can be optimized
      // as a simple mask operation (only keeping the low 32 bits of each
      // 64-bit packed number). By looking at the generated LLVM IR, we can see
      // that LLVM successfully manages to understand & optimize this if we use
      // extractf128, whereas it is not the case with castsi256_si128! The
      // codegen also looks far better, and only use 1-latency cycle
      // instructions.
      return _mm256_extractf128_si256(p, 0);
#else
      return _mm256_castsi256_si128(p);
#endif
    }
    else
    {
      auto[l,h] = v0.slice();
      auto ll = eve::convert(l,tgt);
      auto hh = eve::convert(h,tgt);
      return wide<Out, N>(ll,hh);
    }
  }

  //================================================================================================
  // 128 bits ==> 256 bits
  // This one actually calls stuff. The conversion available are the one that take an AVX
  // sized registers and put it back into a SSE sized register. E.g  converting 4x double AVX
  // will fit nicely into a 4x float AVX.
  //
  // Some conversion are done by hand because they're critical and doesn't exist in AVX/AVX2
  //================================================================================================
  template<real_scalar_value In, typename N, real_scalar_value Out>
  EVE_FORCEINLINE wide<Out, N>
  convert_(EVE_SUPPORTS(avx_), wide<In, N> const &v0, as<Out> const &tgt) noexcept
    requires std::same_as<abi_t<In, N>, x86_128_>
  {
    //==============================================================================================
    // Idempotent call
    //==============================================================================================
    if constexpr( std::is_same_v<In, Out> )
    {
      return v0;
    }
    //==============================================================================================
    // Convert to double
    //==============================================================================================
    else if constexpr( std::is_same_v<Out, double>  && (N::value == 4) )
    {
      if constexpr( std::is_same_v<In, float> )
      {
        return _mm256_cvtps_pd(v0);
      }
      else if constexpr( std::is_same_v<In, std::int32_t> )
      {
        return _mm256_cvtepi32_pd(v0);
      }
      else
      {
        return convert_(EVE_RETARGET(sse4_2_), v0, tgt);
      }
    }
    //==============================================================================================
    // Convert to 16 bits integers
    //==============================================================================================
    else if constexpr( std::is_integral_v<Out> && (sizeof(Out) == 2) && (N::value == 16))
    {
      if constexpr( current_api >= avx2 )
      {
        if constexpr( std::is_integral_v<In> && (sizeof(In) == 1) )
        {
          if constexpr( std::is_signed_v<In> )
          {
            return _mm256_cvtepi8_epi16(v0);
          }
          else
          {
            return _mm256_cvtepu8_epi16(v0);
          }
        }
        else
        {
          return convert_(EVE_RETARGET(sse4_2_), v0, tgt);
        }
      }
      else
      {
        return convert_(EVE_RETARGET(sse4_2_), v0, tgt);
      }
    }
    //==============================================================================================
    // Convert to 32 bits integers
    //==============================================================================================
    else if constexpr( std::is_integral_v<Out> && (sizeof(Out) == 4) && (N::value == 8))
    {
      if constexpr( current_api >= avx2 )
      {
        if constexpr( std::is_integral_v<In> && (sizeof(In) == 1) )
        {
          if constexpr( std::is_signed_v<In> )
          {
            return _mm256_cvtepi8_epi32(v0);
          }
          else
          {
            return _mm256_cvtepu8_epi32(v0);
          }
        }
        else if constexpr( std::is_integral_v<In> && (sizeof(In) == 2) )
        {
          if constexpr( std::is_signed_v<In> )
          {
            return _mm256_cvtepi16_epi32(v0);
          }
          else
          {
            return _mm256_cvtepu16_epi32(v0);
          }
        }
        else
        {
          return convert_(EVE_RETARGET(sse4_2_), v0, tgt);
        }
      }
      else
      {
        return convert_(EVE_RETARGET(sse4_2_), v0, tgt);
      }
    }
    //==============================================================================================
    // Convert to 64 bits integers
    //==============================================================================================
    else if constexpr( std::is_integral_v<Out> && (sizeof(Out) == 8) && (N::value == 4))
    {
      if constexpr( current_api >= avx2 )
      {
        if constexpr( std::is_integral_v<In> && (sizeof(In) == 1) )
        {
          if constexpr( std::is_signed_v<In> )
          {
            return _mm256_cvtepi8_epi64(v0);
          }
          else
          {
            return _mm256_cvtepu8_epi64(v0);
          }
        }
        else if constexpr( std::is_integral_v<In> && (sizeof(In) == 2) )
        {
          if constexpr( std::is_signed_v<In> )
          {
            return _mm256_cvtepi16_epi64(v0);
          }
          else
          {
            return _mm256_cvtepu16_epi64(v0);
          }
        }
        else if constexpr( std::is_integral_v<In> && (sizeof(In) == 4) )
        {
          if constexpr( std::is_signed_v<In> )
          {
            return _mm256_cvtepi32_epi64(v0);
          }
          else
          {
            return _mm256_cvtepu32_epi64(v0);
          }
        }
        else
        {
          return convert_(EVE_RETARGET(sse4_2_), v0, tgt);
        }
      }
      else
      {
        if constexpr( std::is_integral_v<In> && (sizeof(In) == 4) )
        {
          auto[l,h] = v0.slice();
          auto ll = eve::convert(l,tgt);
          auto hh = eve::convert(h,tgt);
          return wide<Out, N>(ll,hh);
        }
        else
        {
          return convert_(EVE_RETARGET(sse4_2_), v0, tgt);
        }
      }
    }
    else
    {
      return convert_(EVE_RETARGET(sse4_2_), v0, tgt);
    }
  }

  template<real_scalar_value In, typename N, real_scalar_value Out>
  EVE_FORCEINLINE logical<wide<Out, N>>
  convert_(EVE_SUPPORTS(sse2_), logical<wide<In, N>> const &v0, as<logical<Out>> const &tgt) noexcept
    requires std::same_as<abi_t<In, N>, x86_256_> && (abi_t<In, N>::is_wide_logical)
  {
    constexpr auto c  = categorize<wide<In, N>>();

    if constexpr ( match(c, category::int16x16, category::uint16x16) && sizeof(Out) == 1 )
    {
      auto [l, h] = v0.slice();
      return _mm_packs_epi16(l, h);
    }
    else return convert_(EVE_RETARGET(simd_), v0, tgt);
  }
}
